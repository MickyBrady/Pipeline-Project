import os
import subprocess

# Define paths
pipeline_dir = "/home/2025/mbrady9/PipelineProject_Michaela_Brady"
log_file_path = "/home/2025/mbrady9/PipelineProject_Michaela_Brady/PipelineProject.log"
 
# Define the FASTA file generated by HCMVgenome1.py
fasta_filename = "HCMV_cds.fasta" 

# Ensure the log file has a header before appending results
def initialize_log():
    # Check if the log file exists and is empty
    if not os.path.exists(log_file_path) or os.stat(log_file_path).st_size == 0:
        with open(log_file_path, "w") as log_file:
            log_file.write("The HCMV genome (NC_006273.2) has 169 CDS.\n\n")  
            # Leave an empty line here
            log_file.write("sample\tcondition\tmin_tpm\tmed_tpm\tmean_tpm\tmax_tpm\n")  
            # Header on line 3

# Function to run a script and capture output
def run_script(script_name, is_first_run=False):
    script_path = os.path.join(pipeline_dir, script_name)

    print(f"\nRunning {script_name}...\n")  # Print to terminal

    with open(log_file_path, "a") as log:  # Open log in append mode
        # Write standard output of the script
        try:
            process = subprocess.run(["python3", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

            # Log output to file only for the first run (TPM results from quantifytpm2.py)
            if is_first_run:  # Only log the TPM results from the first run
                # Remove the "Logged" lines and "Skipping redundant HCMV log entry" from the output (if present)
                cleaned_output = "\n".join(line for line in process.stdout.split("\n") if not line.startswith("Logged") and "Skipping redundant HCMV log entry" not in line)
                log.write(cleaned_output + "\n")

            # Log errors (if any)
            if process.stderr:
                log.write(process.stderr + "\n")

            print(process.stdout)  # Print script output to terminal
            if process.stderr:
                print(f"Error in {script_name}:\n{process.stderr}")
        except Exception as e:
            log.write(f"Error running {script_name}: {e}\n")
            print(f"Error running {script_name}: {e}")  # Print error message to terminal

# Initialize log with the header and HCMV genome info
initialize_log()

# Download the HCMV genome FASTA file using wget
wget_command = f"wget -O {fasta_filename} 'https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id=NC_006273.2&db=nuccore&report=fasta&extrafeat=null&conwithfeat=on&hide-cdd=on'"
subprocess.run(wget_command, shell=True, check=True)

# Run the scripts in the correct order
run_script("HCMVgenome1.py", is_first_run=True)  # Runs first and logs output

# Build the kallisto index
index_command = f"kallisto index -i {os.path.join(pipeline_dir, 'HCMV_index.idx')} {fasta_filename}"
subprocess.run(index_command, shell=True, check=True)

# Define sample FASTQ files
samples = {
    "Donor1_2dpi": ("/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660030_1.fastq", "/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660030_2.fastq"),
    "Donor1_6dpi": ("/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660033_1.fastq", "/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660033_2.fastq"),
    "Donor3_2dpi": ("/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660044_1.fastq", "/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660044_2.fastq"),
    "Donor3_6dpi": ("/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660045_1.fastq", "/home/2025/mbrady9/PipelineProject_Michaela_Brady/sample_data/sample_SRR5660045_2.fastq")
}

# Quantify TPM for each sample
for sample, (fastq1, fastq2) in samples.items():
    output_dir = os.path.join(pipeline_dir, f"Kallisto_Sleuth/{sample}")
    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists
    quant_command = f"kallisto quant -i {os.path.join(pipeline_dir, 'HCMV_index.idx')} -o {output_dir} -b 10 -t 2 {fastq1} {fastq2}"
    subprocess.run(quant_command, shell=True, check=True)

# Run the TPM quantification script
run_script("quantifytpm2.py", is_first_run=True)  # Logs the TPM results for the first time without "Logged" entries

# Create the sample_table.txt file
sample_table_content = """sample    condition    path
SRR5660030    2dpi    /home/2025/mbrady9/PipelineProject_Michaela_Brady/Kallisto_Sleuth/Donor1_2dpi
SRR5660033    6dpi    /home/2025/mbrady9/PipelineProject_Michaela_Brady/Kallisto_Sleuth/Donor1_6dpi
SRR5660044    2dpi    /home/2025/mbrady9/PipelineProject_Michaela_Brady/Kallisto_Sleuth/Donor3_2dpi
SRR5660045    6dpi    /home/2025/mbrady9/PipelineProject_Michaela_Brady/Kallisto_Sleuth/Donor3_6dpi
"""

sample_table_path = os.path.join(pipeline_dir, "sample_table.txt")
with open(sample_table_path, "w") as sample_table_file:
    sample_table_file.write(sample_table_content)

# Run the sleuth analysis in R
r_script = """
# Load necessary libraries
library(sleuth)
library(dplyr)

# Read in the sample table
stab <- read.table('sample_table.txt', header = TRUE)

# Initialize Sleuth object using sleuth_prep function
so <- sleuth_prep(stab)

# Fit a model comparing the two conditions
so <- sleuth_fit(so, ~condition, 'full')

# Fit the reduced model (null model)
so <- sleuth_fit(so, ~1, 'reduced')

# Perform the likelihood ratio test for differential expression
so <- sleuth_lrt(so, 'reduced', 'full')

# Extract the results of the likelihood ratio test
sleuth_table <- sleuth_results(so, 'reduced:full', 'lrt', show_all = FALSE)

# Filter results for significant transcripts (FDR/qval < 0.05)
sleuth_significant <- dplyr::filter(sleuth_table, qval <= 0.05) %>% 
  dplyr::arrange(pval)

# Print top 10 significant transcripts
head(sleuth_significant, n = 10)

# Write results for significant transcripts (FDR < 0.05) to a separate file
write.table(sleuth_significant, file = 'sleuthfdr_results.txt', quote = FALSE, row.names = FALSE, sep = '\\t')

# Need bootstrap results from kallisto for plotting
so = sleuth_prep(stab, extra_bootstrap_summary = TRUE, read_bootstrap_tpm = TRUE)

# Select the most significant transcript (smallest p-value)
top_target <- sleuth_significant$target_id[1]  # Take the first row (lowest pval)

# Check if a significant transcript was found
if (!is.na(top_target)) {
  
  # Generate the bootstrap expression plot
  topplot <- plot_bootstrap(so, top_target, units = 'tpm', color_by = 'condition')
  print(topplot)  # Display in RStudio
  
  # Save the plot dynamically with the transcript ID
  png_filename <- paste0(top_target, '_TPM.png')
  png(png_filename)
  print(topplot)
  dev.off()
  
  # Print confirmation message
  print(paste('Plot saved as:', png_filename))
  
} else {
  print('No significant transcripts found (qval < 0.05).')
}
"""

r_script_path = os.path.join(pipeline_dir, "sleuth_analysis.R")

# Write R script to a file
with open(r_script_path, "w") as r_file:
    r_file.write(r_script)

# Run the R script for sleuth analysis
subprocess.run(f"Rscript {r_script_path}", shell=True, check=True)

# Add Bowtie5.py script execution after all previous steps
run_script("Bowtie5.py", is_first_run=False)  

# Run the final SPAdes assembly step 
spades_command = "bash Spades.sh"  # Assuming Spades.sh exists in the pipeline directory
subprocess.run(spades_command, shell=True, check=True)

# Add the last step in the pipeline to run and print results to log 
run_script("Spadescontig.py", is_first_run=False)
